{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n"
     ]
    }
   ],
   "source": [
    "print('Importing data...')\n",
    "\n",
    "data = pd.read_csv(r'data/application_train.csv')\n",
    "\n",
    "test = pd.read_csv(r'data/application_test.csv')\n",
    "\n",
    "prev = pd.read_csv(r'data/previous_application.csv')\n",
    "\n",
    "buro = pd.read_csv(r'data/bureau.csv')\n",
    "\n",
    "buro_balance = pd.read_csv(r'data/bureau_balance.csv')\n",
    "\n",
    "lgbm_submission = pd.read_csv(r'data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate target variable\n",
    "y = data['TARGET']\n",
    "del data['TARGET']\n",
    "\n",
    "#Feature engineering\n",
    "#One-hot encoding of categorical features in data and test sets\n",
    "\n",
    "categorical_features = [col for col in data.columns if data[col].dtype == 'object']\n",
    "one_hot_df = pd.concat([data,test])\n",
    "one_hot_df = pd.get_dummies(one_hot_df, columns=categorical_features)\n",
    "\n",
    "\n",
    "data = one_hot_df.iloc[:data.shape[0],:]\n",
    "test = one_hot_df.iloc[data.shape[0]:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing buro_balance...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing buro_balance\n",
    "\n",
    "print('Pre-processing buro_balance...')\n",
    "\n",
    "buro_grouped_size = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].size()\n",
    "\n",
    "buro_grouped_max = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].max()\n",
    "\n",
    "buro_grouped_min = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].min()\n",
    "\n",
    "buro_grouped_mean = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].mean()\n",
    "\n",
    "\n",
    "buro_counts = buro_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts(normalize = False)\n",
    "buro_counts_unstacked = buro_counts.unstack('STATUS')\n",
    "buro_counts_unstacked.columns = ['STATUS_0', 'STATUS_1','STATUS_2','STATUS_3','STATUS_4','STATUS_5','STATUS_C','STATUS_X',]\n",
    "buro_counts_unstacked['MONTHS_COUNT'] = buro_grouped_size\n",
    "buro_counts_unstacked['MONTHS_MIN'] = buro_grouped_min\n",
    "buro_counts_unstacked['MONTHS_MAX'] = buro_grouped_max\n",
    "buro_counts_unstacked['MONTHS_MEAN'] = buro_grouped_mean\n",
    "\n",
    "\n",
    "\n",
    "buro1 = buro.join(buro_counts_unstacked.fillna(0), how='left', on='SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1716428, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing previous_application...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing previous_application\n",
    "\n",
    "print('Pre-processing previous_application...')\n",
    "\n",
    "#One-hot encoding of categorical features in previous application data set\n",
    "\n",
    "prev_cat_features = [pcol for pcol in prev.columns if prev[pcol].dtype == 'object']\n",
    "\n",
    "prev = pd.get_dummies(prev, columns=prev_cat_features)\n",
    "\n",
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "avg_prev2 = prev.groupby('SK_ID_CURR').min()\n",
    "avg_prev3 = prev.groupby('SK_ID_CURR').max()\n",
    "\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "\n",
    "del avg_prev['SK_ID_PREV'], avg_prev2['SK_ID_PREV'], avg_prev3['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing buro...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing buro\n",
    "\n",
    "print('Pre-processing buro...')\n",
    "\n",
    "#One-hot encoding of categorical features in buro data set\n",
    "\n",
    "buro_cat_features = [bcol for bcol in buro1.columns if buro1[bcol].dtype == 'object']\n",
    "\n",
    "buro1 = pd.get_dummies(buro, columns=buro_cat_features)\n",
    "\n",
    "avg_buro = buro1.groupby('SK_ID_CURR').mean()\n",
    "avg_buro2 = buro1.groupby('SK_ID_CURR').min()\n",
    "avg_buro3 = buro1.groupby('SK_ID_CURR').max()\n",
    "\n",
    "avg_buro['buro_count'] = buro1[['SK_ID_BUREAU', 'SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "\n",
    "del avg_buro['SK_ID_BUREAU'], avg_buro2['SK_ID_BUREAU'], avg_buro3['SK_ID_BUREAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing POS_CASH...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing POS_CASH\n",
    "\n",
    "print('Pre-processing POS_CASH...')\n",
    "POS_CASH  = pd.read_csv(r'data/POS_CASH_balance.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "POS_CASH['NAME_CONTRACT_STATUS'] = le.fit_transform(POS_CASH['NAME_CONTRACT_STATUS'].astype(str))\n",
    "\n",
    "nunique_status = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "\n",
    "nunique_status2 = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\n",
    "\n",
    "nunique_status3 = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').min()\n",
    "\n",
    "POS_CASH['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "POS_CASH['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\n",
    "POS_CASH['NUNIQUE_STATUS3'] = nunique_status3['NAME_CONTRACT_STATUS']\n",
    "\n",
    "POS_CASH.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing credit_card...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing credit_card\n",
    "\n",
    "print('Pre-processing credit_card...')\n",
    "\n",
    "credit_card  = pd.read_csv(r'data/credit_card_balance.csv')\n",
    "\n",
    "credit_card['NAME_CONTRACT_STATUS'] = le.fit_transform(credit_card['NAME_CONTRACT_STATUS'].astype(str))\n",
    "\n",
    "nunique_status = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "\n",
    "nunique_status2 = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\n",
    "\n",
    "nunique_status3 = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').min()\n",
    "\n",
    "nunique_status4 = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').mean()\n",
    "\n",
    "credit_card['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "\n",
    "credit_card['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\n",
    "\n",
    "credit_card['NUNIQUE_STATUS3'] = nunique_status3['NAME_CONTRACT_STATUS']\n",
    "\n",
    "credit_card['NUNIQUE_STATUS4'] = nunique_status4['NAME_CONTRACT_STATUS']\n",
    "\n",
    "credit_card.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing payments...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing payments\n",
    "\n",
    "print('Pre-processing payments...')\n",
    "\n",
    "payments = pd.read_csv(r'data/installments_payments.csv')\n",
    "\n",
    "payments['PAYMENT_PERC'] = payments['AMT_PAYMENT'] / payments['AMT_INSTALMENT']\n",
    "payments['PAYMENT_DIFF'] = payments['AMT_INSTALMENT'] - payments['AMT_PAYMENT']\n",
    "\n",
    "# Days past due and days before due (no negative values)\n",
    "payments['DPD'] = payments['DAYS_ENTRY_PAYMENT'] - payments['DAYS_INSTALMENT']\n",
    "payments['DBD'] = payments['DAYS_INSTALMENT'] - payments['DAYS_ENTRY_PAYMENT']\n",
    "payments['DPD'] = payments['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "payments['DBD'] = payments['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "avg_payments = payments.groupby('SK_ID_CURR').mean()\n",
    "\n",
    "avg_payments2 = payments.groupby('SK_ID_CURR').max()\n",
    "\n",
    "avg_payments3 = payments.groupby('SK_ID_CURR').min()\n",
    "\n",
    "avg_payments4 = payments.groupby('SK_ID_CURR').sum()\n",
    "\n",
    "del avg_payments['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining databases...\n"
     ]
    }
   ],
   "source": [
    "#Join data bases\n",
    "\n",
    "print('Joining databases...')\n",
    "\n",
    "data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_buro2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_buro2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_buro3.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_buro3.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "\n",
    "data = data.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_payments4.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_payments4.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature engineering...\n"
     ]
    }
   ],
   "source": [
    "print('Some feature engineering...')\n",
    "\n",
    "data['DAYS_EMPLOYED_PER_AGE'] = data['DAYS_EMPLOYED']/data['DAYS_BIRTH']\n",
    "data['AMT_CREDIT_PER_INCOME'] = data['AMT_INCOME_TOTAL']/data['AMT_CREDIT_x']\n",
    "data['INCOME_PER_PERSON'] = data['AMT_INCOME_TOTAL'] / data['CNT_FAM_MEMBERS']\n",
    "data['ANNUITY_INCOME_PERC'] = data['AMT_ANNUITY'] / data['AMT_INCOME_TOTAL']\n",
    "data['PAYMENT_RATE'] = data['AMT_ANNUITY'] / data['AMT_CREDIT_x']\n",
    "\n",
    "test['DAYS_EMPLOYED_PER_AGE'] = test['DAYS_EMPLOYED']/test['DAYS_BIRTH']\n",
    "test['AMT_CREDIT_PER_INCOME'] = test['AMT_INCOME_TOTAL']/test['AMT_CREDIT_x']\n",
    "test['INCOME_PER_PERSON'] = test['AMT_INCOME_TOTAL'] / test['CNT_FAM_MEMBERS']\n",
    "test['ANNUITY_INCOME_PERC'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "test['PAYMENT_RATE'] = test['AMT_ANNUITY'] / test['AMT_CREDIT_x']\n",
    "\n",
    "#data['APP_CREDIT_PERC'] = data['AMT_APPLICATION']/data['AMT_ANNUITY_x']\n",
    "\n",
    "data['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "data['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "data['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "data['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "data['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "data['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "test['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "test['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "test['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "test['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "test['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "test['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing features with more than 80% missing...\n"
     ]
    }
   ],
   "source": [
    "#Remove features with many missing values\n",
    "\n",
    "print('Removing features with more than 80% missing...')\n",
    "\n",
    "test = test[test.columns[data.isnull().mean() < 0.85]]\n",
    "\n",
    "data = data[data.columns[data.isnull().mean() < 0.85]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_feats = ['SK_ID_CURR']\n",
    "\n",
    "features = [f_ for f_ in data.columns if f_ not in excluded_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "weights = compute_class_weight('balanced', np.unique(y), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54390914,  6.19357503])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.772096\tvalid_1's auc: 0.758585\n",
      "[200]\ttraining's auc: 0.794025\tvalid_1's auc: 0.773256\n",
      "[300]\ttraining's auc: 0.808033\tvalid_1's auc: 0.780151\n",
      "[400]\ttraining's auc: 0.818558\tvalid_1's auc: 0.783209\n",
      "[500]\ttraining's auc: 0.827663\tvalid_1's auc: 0.7851\n",
      "[600]\ttraining's auc: 0.835681\tvalid_1's auc: 0.786117\n",
      "[700]\ttraining's auc: 0.843287\tvalid_1's auc: 0.786719\n",
      "[800]\ttraining's auc: 0.850339\tvalid_1's auc: 0.786943\n",
      "[900]\ttraining's auc: 0.856819\tvalid_1's auc: 0.787268\n",
      "[1000]\ttraining's auc: 0.863\tvalid_1's auc: 0.787491\n",
      "[1100]\ttraining's auc: 0.868782\tvalid_1's auc: 0.787438\n",
      "Early stopping, best iteration is:\n",
      "[1002]\ttraining's auc: 0.86311\tvalid_1's auc: 0.787493\n",
      "Fold  1 AUC : 0.787493\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.772491\tvalid_1's auc: 0.753039\n",
      "[200]\ttraining's auc: 0.795304\tvalid_1's auc: 0.768511\n",
      "[300]\ttraining's auc: 0.809172\tvalid_1's auc: 0.775463\n",
      "[400]\ttraining's auc: 0.819472\tvalid_1's auc: 0.779185\n",
      "[500]\ttraining's auc: 0.828249\tvalid_1's auc: 0.781564\n",
      "[600]\ttraining's auc: 0.836358\tvalid_1's auc: 0.783132\n",
      "[700]\ttraining's auc: 0.84392\tvalid_1's auc: 0.78402\n",
      "[800]\ttraining's auc: 0.850938\tvalid_1's auc: 0.78472\n",
      "[900]\ttraining's auc: 0.857415\tvalid_1's auc: 0.785097\n",
      "[1000]\ttraining's auc: 0.863613\tvalid_1's auc: 0.785398\n",
      "[1100]\ttraining's auc: 0.869476\tvalid_1's auc: 0.785613\n",
      "[1200]\ttraining's auc: 0.875028\tvalid_1's auc: 0.785744\n",
      "[1300]\ttraining's auc: 0.880258\tvalid_1's auc: 0.785965\n",
      "[1400]\ttraining's auc: 0.88535\tvalid_1's auc: 0.786194\n",
      "[1500]\ttraining's auc: 0.890168\tvalid_1's auc: 0.786305\n",
      "[1600]\ttraining's auc: 0.894707\tvalid_1's auc: 0.786231\n",
      "Early stopping, best iteration is:\n",
      "[1520]\ttraining's auc: 0.891077\tvalid_1's auc: 0.786355\n",
      "Fold  2 AUC : 0.786355\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.772243\tvalid_1's auc: 0.759629\n",
      "[200]\ttraining's auc: 0.794039\tvalid_1's auc: 0.773587\n",
      "[300]\ttraining's auc: 0.80812\tvalid_1's auc: 0.780492\n",
      "[400]\ttraining's auc: 0.818733\tvalid_1's auc: 0.78395\n",
      "[500]\ttraining's auc: 0.82786\tvalid_1's auc: 0.785976\n",
      "[600]\ttraining's auc: 0.836175\tvalid_1's auc: 0.787146\n",
      "[700]\ttraining's auc: 0.843841\tvalid_1's auc: 0.787956\n",
      "[800]\ttraining's auc: 0.850902\tvalid_1's auc: 0.788247\n",
      "[900]\ttraining's auc: 0.857553\tvalid_1's auc: 0.788625\n",
      "[1000]\ttraining's auc: 0.863774\tvalid_1's auc: 0.78888\n",
      "[1100]\ttraining's auc: 0.86966\tvalid_1's auc: 0.788919\n",
      "[1200]\ttraining's auc: 0.875268\tvalid_1's auc: 0.789056\n",
      "Early stopping, best iteration is:\n",
      "[1187]\ttraining's auc: 0.874541\tvalid_1's auc: 0.789095\n",
      "Fold  3 AUC : 0.789095\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.773123\tvalid_1's auc: 0.756673\n",
      "[200]\ttraining's auc: 0.795138\tvalid_1's auc: 0.771\n",
      "[300]\ttraining's auc: 0.808868\tvalid_1's auc: 0.777798\n",
      "[400]\ttraining's auc: 0.819368\tvalid_1's auc: 0.781225\n",
      "[500]\ttraining's auc: 0.82843\tvalid_1's auc: 0.783168\n",
      "[600]\ttraining's auc: 0.836632\tvalid_1's auc: 0.784169\n",
      "[700]\ttraining's auc: 0.844108\tvalid_1's auc: 0.784902\n",
      "[800]\ttraining's auc: 0.851145\tvalid_1's auc: 0.785312\n",
      "[900]\ttraining's auc: 0.857813\tvalid_1's auc: 0.785436\n",
      "[1000]\ttraining's auc: 0.86401\tvalid_1's auc: 0.785537\n",
      "[1100]\ttraining's auc: 0.869831\tvalid_1's auc: 0.785593\n",
      "Early stopping, best iteration is:\n",
      "[1057]\ttraining's auc: 0.867387\tvalid_1's auc: 0.785716\n",
      "Fold  4 AUC : 0.785716\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.77249\tvalid_1's auc: 0.756212\n",
      "[200]\ttraining's auc: 0.794823\tvalid_1's auc: 0.770311\n",
      "[300]\ttraining's auc: 0.808799\tvalid_1's auc: 0.776723\n",
      "[400]\ttraining's auc: 0.819313\tvalid_1's auc: 0.779897\n",
      "[500]\ttraining's auc: 0.828467\tvalid_1's auc: 0.781738\n",
      "[600]\ttraining's auc: 0.836527\tvalid_1's auc: 0.782572\n",
      "[700]\ttraining's auc: 0.844024\tvalid_1's auc: 0.783034\n",
      "[800]\ttraining's auc: 0.851127\tvalid_1's auc: 0.783392\n",
      "[900]\ttraining's auc: 0.857689\tvalid_1's auc: 0.783469\n",
      "[1000]\ttraining's auc: 0.863996\tvalid_1's auc: 0.783498\n",
      "Early stopping, best iteration is:\n",
      "[977]\ttraining's auc: 0.862561\tvalid_1's auc: 0.783629\n",
      "Fold  5 AUC : 0.783629\n",
      "Full AUC score 0.786329 using LGBM\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle = True)\n",
    "oof_preds_lgbm = np.zeros(data.shape[0])\n",
    "sub_preds_lgbm = np.zeros(test.shape[0])\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(data)):\n",
    "    trn_x, trn_y = data[features].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = data[features].iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf = LGBMClassifier(\n",
    "            is_unbalance = True, \n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=30,\n",
    "            colsample_bytree=.8,\n",
    "            subsample=.9,\n",
    "            max_depth=8,\n",
    "            reg_alpha=.1,\n",
    "            reg_lambda=.1,\n",
    "            min_split_gain=.01,\n",
    "            min_child_weight=2,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "    \n",
    "    clf.fit(trn_x, trn_y, eval_set =  [(trn_x, trn_y), (val_x, val_y)], eval_metric='auc', verbose=100, early_stopping_rounds=100)\n",
    "    \n",
    "    oof_preds_lgbm[val_idx] = clf.predict_proba(val_x)[:, 1]\n",
    "\n",
    "    sub_preds_lgbm += clf.predict_proba(test[features])[:, 1] / folds.n_splits\n",
    "\n",
    "    \n",
    "\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds_lgbm[val_idx])))\n",
    "\n",
    "    del clf, trn_x, trn_y, val_x, val_y\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Full AUC score %.6f using LGBM' % roc_auc_score(y, oof_preds_lgbm))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full AUC score 0.786329 using bagging XGB and LGBM\n"
     ]
    }
   ],
   "source": [
    "print('Full AUC score %.6f using bagging XGB and LGBM' % roc_auc_score(y, oof_preds_lgbm))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95547940510565998"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['TARGET'] = sub_preds_lgbm\n",
    "test['TARGET'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[['SK_ID_CURR','TARGET']].to_csv('lgb_submission_esi.csv', index=False, float_format='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=4, shuffle = True)\n",
    "oof_preds = np.zeros(data.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(data)):\n",
    "    trn_x, trn_y = data[features].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = data[features].iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "         objective = 'binary:logistic', \n",
    "\n",
    "        booster = \"gbtree\",\n",
    "\n",
    "        eval_metric = 'auc', \n",
    "\n",
    "        nthread = 4,\n",
    "\n",
    "        eta = 0.05,\n",
    "\n",
    "        gamma = 0,\n",
    "\n",
    "        max_depth = 6, \n",
    "\n",
    "        subsample = 0.7, \n",
    "\n",
    "        colsample_bytree = 0.7, \n",
    "\n",
    "        colsample_bylevel = 0.675,\n",
    "\n",
    "        min_child_weight = 22,\n",
    "\n",
    "        alpha = 0,\n",
    "\n",
    "        random_state = 42, \n",
    "\n",
    "        nrounds = 3000 , \n",
    "        )\n",
    "    \n",
    "    clf.fit(trn_x, trn_y, eval_set =  [(trn_x, trn_y), (val_x, val_y)], verbose=10, early_stopping_rounds=30)\n",
    "    \n",
    "    oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n",
    "\n",
    "    sub_preds += clf.predict_proba(test[features])[:, 1] / folds.n_splits\n",
    "\n",
    "    \n",
    "\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "\n",
    "    del clf, trn_x, trn_y, val_x, val_y\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Full AUC score %.6f using XGB' % roc_auc_score(y, oof_preds))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Convolution1D, Dense, MaxPool1D, BatchNormalization, Embedding\n",
    "\n",
    "\n",
    "loan = Input(shape=(data.shape[1],))\n",
    "\n",
    "embed = Embedding(input_dim = data.shape[0], \n",
    "                    output_dim = 256)(loan)\n",
    "\n",
    "conv = Convolution1D(96,5,\n",
    "                       strides = 2, \n",
    "                       activation = 'relu')(embed)\n",
    "\n",
    "pool = MaxPool1D(3,\n",
    "                    strides = 2,\n",
    "                   )(conv)\n",
    "\n",
    "flatten = Flatten()(pool)\n",
    "\n",
    "fc1 = Dense(512, \n",
    "               activation = 'relu'\n",
    "               )(flatten)\n",
    "\n",
    "fc2 =  Dense(1,\n",
    "                activation = 'sigmoid')(fc1)\n",
    "\n",
    "model = Model(loan, fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', \n",
    "             optimizer = 'adam', \n",
    "              metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(data,y,\n",
    "          batch_size=128, \n",
    "          validation_split=0.2, \n",
    "         epochs = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dl = np.zeros((data.shape[0],data.shape[1],1))\n",
    "data_dl[:,:,0] = data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
